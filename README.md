# üìä NPM Metrics Scraper

## Arquitectura del Proyecto

```
npm-metrics-scraper/
‚îú‚îÄ‚îÄ npm-metrics-package/          # Carpeta principal del proyecto Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ npm_metrics_package/      # Paquete Python con el c√≥digo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spiders/              # üï∑Ô∏è Los "robots" que hacen el scraping
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package_info_spider.py  # Spider principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py              # üìã Define qu√© datos queremos capturar
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py          # üîß Procesa los datos descargados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py           # ‚öôÔ∏è Configuraci√≥n del scraper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middlewares.py        # üîå Middleware (no usado actualmente)
‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg                # Configuraci√≥n de Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Dependencias del proyecto
‚îÇ   ‚îî‚îÄ‚îÄ npm_metrics_results.json  # üíæ Resultados generados
‚îî‚îÄ‚îÄ README.md                     # üìñ Este archivo
```

## Flujo de Trabajo

### **PASO A: Descubrimiento** üîç
El spider comienza con una lista de paquetes (por ejemplo: `react`, `axios`, `lodash`).

```python
package_list = ['react', 'axios', 'lodash']
```

### **PASO B: API de Descargas** üìä
Para cada paquete, consulta la API de NPM para obtener las **descargas del √∫ltimo mes**.

```
üì° https://api.npmjs.org/downloads/point/last-month/react
```

### **PASO C: API de Registro (Metadata)** üìù
Luego consulta la Registry API de NPM para obtener informaci√≥n detallada:

```
üì° https://registry.npmjs.org/react
```

**Datos que obtiene:**
- ‚úÖ Versi√≥n actual (`version`)
- ‚úÖ Descripci√≥n/prop√≥sito (`purpose`)
- ‚úÖ Dependencias (`dependencies`)
- ‚úÖ Tama√±o descomprimido (`size_mb`)
- ‚úÖ Licencia (`license`)
- ‚úÖ N√∫mero de mantenedores (`maintainer_count`)
- ‚úÖ √öltima modificaci√≥n (`last_modified`)
- ‚úÖ URL del tarball (para descarga)
- ‚úÖ URL p√∫blica del paquete

### **PASO D: An√°lisis Local de C√≥digo** üî¨
El Pipeline descarga el archivo `.tgz` del paquete y analiza:

1. **Descarga el tarball** (archivo comprimido del paquete)
2. **Descomprime** en una carpeta temporal
3. **Cuenta archivos** JavaScript/TypeScript (`.js`, `.ts`, `.jsx`, `.tsx`)
4. **Simula conteo de funciones** (actualmente es una simulaci√≥n)
5. **Limpia** la carpeta temporal

> ‚ö†Ô∏è **Nota:** El conteo de funciones es actualmente una **simulaci√≥n**. Una implementaci√≥n real usar√≠a un parser AST (Abstract Syntax Tree) como `esprima` o `babel-parser`.

## üìã Datos Capturados

Para cada paquete NPM, el scraper recopila:

| Campo | Descripci√≥n | Ejemplo |
|-------|-------------|---------|
| `package_name` | Nombre del paquete | `"react"` |
| `public_url` | URL p√∫blica en npmjs.com | `"https://www.npmjs.com/package/react"` |
| `purpose` | Descripci√≥n corta | `"React is a JavaScript library..."` |
| `downloads_last_month` | Descargas del √∫ltimo mes | `195209356` |
| `version` | Versi√≥n actual | `"19.2.0"` |
| `size_mb` | Tama√±o descomprimido en MB | `0.16` |
| `dependencies` | Dependencias del paquete | `{}` o `{"lodash": "^4.17.0"}` |
| `license` | Tipo de licencia | `"MIT"` |
| `maintainer_count` | N√∫mero de mantenedores | `2` |
| `last_modified` | Fecha de √∫ltima modificaci√≥n | `"2025-10-30T16:21:05.788Z"` |
| `total_files` | Archivos JS/TS en el paquete | `24` |
| `total_functions` | Funciones detectadas (simulado) | `84` |
| `tarball_url` | URL del archivo comprimido | `"https://registry.npmjs.org/react/..."` |

## üöÄ Instalaci√≥n

### Requisitos Previos
- Python 3.11 o superior
- pip (gestor de paquetes de Python)

### Pasos de Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone https://github.com/AnaisRodriguez1/npm-metrics-scraper.git
cd npm-metrics-scraper
```

2. **Crear entorno virtual** (recomendado)
```bash
python -m venv .venv
```

3. **Activar el entorno virtual**

En Windows (PowerShell):
```powershell
.venv\Scripts\Activate.ps1
```

En Windows (CMD):
```cmd
.venv\Scripts\activate.bat
```

En Linux/Mac:
```bash
source .venv/bin/activate
```

4. **Instalar dependencias**
```bash
cd npm-metrics-package
pip install -r requirements.txt
```

## üíª Uso

### Ejecutar el Scraper

```bash
cd npm-metrics-package
scrapy crawl package_info_spider -o npm_metrics_results.json
```

### Cambiar los Paquetes a Analizar

Edita el archivo `npm_metrics_package/spiders/package_info_spider.py`:

```python
package_list = ['react', 'vue', 'angular', 'express', 'next']
```

### Ver los Resultados

Los resultados se guardan en `npm_metrics_results.json`:

## ‚öôÔ∏è Configuraci√≥n

### Ajustar la Velocidad del Scraping

En `npm_metrics_package/settings.py`:

```python
# Espera 1 segundo entre cada request (evita saturar el servidor)
DOWNLOAD_DELAY = 1

# Solo 1 request simult√°neo por dominio
CONCURRENT_REQUESTS_PER_DOMAIN = 1
```

### Cambiar el User-Agent

```python
USER_AGENT = 'npm_metrics_package (tu-email@ejemplo.com)'
```

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **[Scrapy 2.13](https://scrapy.org/)** - Framework de web scraping
- **[Python 3.11](https://www.python.org/)** - Lenguaje de programaci√≥n
- **[Requests](https://requests.readthedocs.io/)** - Para descargar tarballs
- **APIs de NPM:**
  - Downloads API: `api.npmjs.org`
  - Registry API: `registry.npmjs.org`

## üîÆ Pr√≥ximas Mejoras

### An√°lisis de C√≥digo Real
Actualmente, el conteo de funciones es una **simulaci√≥n**. Para implementar an√°lisis real:

```bash
pip install esprima  # Parser JavaScript
```

Luego modificar `pipelines.py` para usar un parser AST real.

### M√©tricas Adicionales Sugeridas
- üìà Tendencia de descargas (√∫ltimos 6 meses)
- üêõ N√∫mero de issues abiertas en GitHub
- ‚≠ê Estrellas en GitHub
- üîÑ Frecuencia de actualizaciones
- üì¶ N√∫mero de versiones publicadas

### Base de Datos
Guardar resultados en una base de datos (SQLite, PostgreSQL, MongoDB) en lugar de solo JSON.
